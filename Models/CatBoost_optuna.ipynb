{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from timeit import default_timer as timer\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "path = Path.cwd().parent / \"mdsb-2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_temporal(X, y, delta_threshold=\"60 days\"):\n",
    "\n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = X[\"date\"] <= cutoff_date\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags(X, cols_to_lag=[\"t\", \"u\", \"vv\", \"nnuage4\"], lag_list=[2, -24, -2]):\n",
    "    X = X.copy()\n",
    "\n",
    "    feature_columns = [col for col in X.columns if col in cols_to_lag]\n",
    "\n",
    "    for l in lag_list:\n",
    "        lag_columns = [f\"{col}_lag{l}\" for col in feature_columns]\n",
    "        X[lag_columns] = X[feature_columns].shift(periods=l, axis=0)\n",
    "        X[lag_columns] = (\n",
    "            X[lag_columns]\n",
    "            .interpolate(method=\"linear\")\n",
    "            .interpolate(method=\"bfill\")\n",
    "            .interpolate(method=\"ffill\")\n",
    "        )\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_moving_average(\n",
    "    X, cols_to_ma=[\"t\", \"u\", \"vv\", \"nnuage4\"], window_list=[24 * 7, 24], centered=True\n",
    "):\n",
    "    X = X.copy()\n",
    "\n",
    "    feature_columns = [col for col in X.columns if col in cols_to_ma]\n",
    "\n",
    "    for w in window_list:\n",
    "        ma_columns = [f\"{col}_ma{w}\" for col in feature_columns]\n",
    "        X[ma_columns] = X[feature_columns].rolling(window=w, center=centered).mean()\n",
    "        X[ma_columns] = (\n",
    "            X[ma_columns]\n",
    "            .interpolate(method=\"linear\")\n",
    "            .interpolate(method=\"bfill\")\n",
    "            .interpolate(method=\"ffill\")\n",
    "        )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf841c63",
   "metadata": {},
   "source": [
    "### Define pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e03f1-dd44-4e94-8b1f-564c3bb8268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dates(X, col_name=\"date\"):\n",
    "    X = X.copy()\n",
    "\n",
    "    X[\"month\"] = X[col_name].dt.month\n",
    "    X[\"weekday\"] = X[col_name].dt.weekday\n",
    "    X[\"hour\"] = X[col_name].dt.hour\n",
    "\n",
    "    X[\"month_sin\"] = np.sin(2 * np.pi * X[\"date\"].dt.month / 12)\n",
    "    X[\"month_cos\"] = np.cos(2 * np.pi * X[\"date\"].dt.month / 12)\n",
    "\n",
    "    X[\"day_sin\"] = np.sin(2 * np.pi * X[\"date\"].dt.day / X[\"date\"].dt.days_in_month)\n",
    "    X[\"day_cos\"] = np.cos(2 * np.pi * X[\"date\"].dt.day / X[\"date\"].dt.days_in_month)\n",
    "\n",
    "    X[\"hour_sin\"] = np.sin(2 * np.pi * X[\"date\"].dt.hour / 24)\n",
    "    X[\"hour_cos\"] = np.cos(2 * np.pi * X[\"date\"].dt.hour / 24)\n",
    "\n",
    "    X[[\"month\", \"weekday\", \"hour\"]] = X[[\"month\", \"weekday\", \"hour\"]].astype(\"category\")\n",
    "\n",
    "    return X.drop(columns=[col_name])\n",
    "\n",
    "\n",
    "def _encode_covid(X, col_name=\"date\"):\n",
    "    X = X.copy()\n",
    "\n",
    "    # Create masks for lockdown dates\n",
    "    lockdown_1 = (X[\"date\"] >= \"2020-10-17\") & (X[\"date\"] <= \"2020-12-14\")\n",
    "\n",
    "    lockdown_2 = (X[\"date\"] >= \"2020-12-15\") & (X[\"date\"] <= \"2021-02-26\")\n",
    "\n",
    "    lockdown_3 = (X[\"date\"] >= \"2021-02-27\") & (X[\"date\"] <= \"2021-05-02\")\n",
    "\n",
    "    X[\"Covid\"] = 0\n",
    "    X.loc[lockdown_1 | lockdown_2 | lockdown_3, \"Covid\"] = 1\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def _merge_external_data(X, include_lags=True, include_ma=True):\n",
    "    to_keep = [\n",
    "        \"date\",\n",
    "        \"hnuage4\",\n",
    "        \"t\",\n",
    "        \"ctype4\",\n",
    "        \"nnuage4\",\n",
    "        \"u\",\n",
    "        \"etat_sol\",\n",
    "        \"perssfrai\",\n",
    "        \"tx12\",\n",
    "        \"cm\",\n",
    "        \"tn12\",\n",
    "        \"tend24\",\n",
    "        \"vv\",\n",
    "        \"rafper\",\n",
    "        \"rr24\",\n",
    "        \"hnuage2\",\n",
    "        \"td\",\n",
    "        \"rr3\",\n",
    "        \"hnuage3\",\n",
    "        \"hnuage1\",\n",
    "    ]\n",
    "\n",
    "    ext_data = pd.read_csv(path / \"external_data.csv\", parse_dates=[\"date\"])[to_keep]\n",
    "\n",
    "    ext_data.drop(columns=ext_data.columns[ext_data.isna().sum() > 1000], inplace=True)\n",
    "\n",
    "    full_date_range = pd.date_range(\n",
    "        start=np.min([np.min(data.date), np.min(test.date)]),\n",
    "        end=np.max([np.max(data.date), np.max(test.date)]),\n",
    "        freq=\"H\",\n",
    "    )\n",
    "\n",
    "    full_date_range = pd.DataFrame({\"date\": full_date_range})\n",
    "\n",
    "    ext_data = full_date_range.merge(ext_data, on=\"date\", how=\"left\")\n",
    "\n",
    "    columns_to_interpolate = ext_data.drop(columns=\"date\").columns\n",
    "    ext_data[columns_to_interpolate] = (\n",
    "        ext_data[columns_to_interpolate]\n",
    "        .interpolate(method=\"polynomial\", order=3)\n",
    "        .interpolate(method=\"bfill\")\n",
    "        .interpolate(method=\"ffill\")\n",
    "    )\n",
    "\n",
    "    if include_lags:\n",
    "        ext_data = add_lags(ext_data)\n",
    "\n",
    "    if include_ma:\n",
    "        ext_data = add_moving_average(ext_data)\n",
    "\n",
    "    to_drop = [\n",
    "        \"vv_ma24\",\n",
    "        \"rr24\",\n",
    "        \"t_lag2\",\n",
    "        \"rafper\",\n",
    "        \"hnuage1\",\n",
    "        \"td\",\n",
    "        \"vv\",\n",
    "        \"perssfrai\",\n",
    "        \"vv_lag2\",\n",
    "        \"u_lag-24\",\n",
    "        \"vv_lag-2\",\n",
    "        \"vv_lag-24\",\n",
    "        \"u\",\n",
    "        \"u_lag2\",\n",
    "    ]\n",
    "\n",
    "    ext_data.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "    X = X.copy()\n",
    "\n",
    "    X[\"date\"] = X[\"date\"].astype(\"datetime64[ns]\")\n",
    "    ext_data[\"date\"] = ext_data[\"date\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "    X[\"orig_index\"] = np.arange(X.shape[0])\n",
    "\n",
    "    X = pd.merge_asof(X.sort_values(\"date\"), ext_data.sort_values(\"date\"), on=\"date\")\n",
    "\n",
    "    # Sort back to the original order\n",
    "    X = X.sort_values(\"orig_index\")\n",
    "    del X[\"orig_index\"]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def _gas_price_encoder(X):\n",
    "    X = X.copy()\n",
    "    X[\"gas_price\"] = 1\n",
    "\n",
    "    gas_prices = np.array(\n",
    "        [\n",
    "            1.22,\n",
    "            1.21,\n",
    "            1.22,\n",
    "            1.27,\n",
    "            1.31,\n",
    "            1.36,\n",
    "            1.4,\n",
    "            1.39,\n",
    "            1.4,\n",
    "            1.43,\n",
    "            1.45,\n",
    "            1.45,\n",
    "            1.46,\n",
    "            1.56,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    years = [\n",
    "        2020,\n",
    "        2020,\n",
    "        2020,\n",
    "        2020,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "        2021,\n",
    "    ]\n",
    "\n",
    "    months = [9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    for i, price in enumerate(gas_prices):\n",
    "        X.loc[\n",
    "            (X.date.dt.month == months[i]) & (X.date.dt.year == years[i]), \"gas_price\"\n",
    "        ] = price\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def full_encode(X):\n",
    "    return _encode_dates(_encode_covid(_gas_price_encoder(_merge_external_data(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd8529",
   "metadata": {},
   "source": [
    "## Import main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e43506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(path / \"train.parquet\")\n",
    "test = pd.read_parquet(path / \"final_test.parquet\")\n",
    "\n",
    "targets = [\"bike_count\", \"log_bike_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd5672-8be2-4a14-9578-5b46a8c14103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\n",
    "    columns=[\n",
    "        \"site_name\",\n",
    "        \"counter_id\",\n",
    "        \"site_id\",\n",
    "        \"counter_installation_date\",\n",
    "        \"coordinates\",\n",
    "        \"counter_technical_id\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6adcf7-f69e-451e-a65f-0028256d8361",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optuna def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834fba7-e9e3-425d-9236-5260568914aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.1, 0.25)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 9)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 250, 1000)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.7, 0.95)\n",
    "\n",
    "    data_merger = FunctionTransformer(_merge_external_data, validate=False)\n",
    "    covid_encoder = FunctionTransformer(_encode_covid, validate=False)\n",
    "    gas_encoder = FunctionTransformer(_gas_price_encoder, validate=False)\n",
    "    date_encoder = FunctionTransformer(_encode_dates, validate=False)\n",
    "\n",
    "    regressor = CatBoostRegressor(\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        subsample=subsample,\n",
    "        od_pval=1e-5,\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"merge external\", data_merger),\n",
    "            (\"gas prices encoder\", gas_encoder),\n",
    "            (\"covid encoder\", covid_encoder),\n",
    "            (\"date encoder\", date_encoder),\n",
    "            (\"regressor\", regressor),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Perform temporal train-test split\n",
    "    X_train, y_train, X_valid, y_valid = train_test_split_temporal(X, y)\n",
    "\n",
    "    val_pool = Pool(\n",
    "        full_encode(X_valid),\n",
    "        label=y_valid,\n",
    "        cat_features=categorical_cols,\n",
    "    )\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipe.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        regressor__cat_features=categorical_cols,\n",
    "        regressor__early_stopping_rounds=130,\n",
    "        regressor__eval_set=val_pool,\n",
    "    )\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = pipe.predict(X_valid)\n",
    "\n",
    "    # Calculate the mean squared error as the objective\n",
    "    neg_rmse = -mean_squared_error(y_valid, predictions, squared=False)\n",
    "\n",
    "    return neg_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c2b95",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c372553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = data.drop(columns=targets), data[\"log_bike_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e4d97-8f96-4cb0-b1d0-4514ed179496",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = (\n",
    "    _encode_dates(X[[\"date\"]]).select_dtypes(include=\"category\").columns.tolist()\n",
    ")\n",
    "categorical_cols = [\"counter_name\"] + date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79d587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get the best parameters from the study\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151056ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param1 = list(param_grid.keys())[0]\n",
    "# param2 = list(param_grid.keys())[1]\n",
    "# p1_name = param1.split('__')[1]\n",
    "# p2_name = param2.split('__')[1]\n",
    "\n",
    "# # Extract the relevant information for plotting vs the max_depth parameter\n",
    "# mean_test_scores_depth = np.sqrt(-grid_search.cv_results_['mean_test_score'].reshape(len(param_grid[param1]), -1))\n",
    "# n_estimators_values = param_grid[param2]\n",
    "\n",
    "# # Extract the relevant information for plotting vs the n_estimators parameter\n",
    "# mean_test_scores_estimators = np.sqrt(-grid_search.cv_results_['mean_test_score'].reshape(len(param_grid[param1]), -1)).T\n",
    "# max_depth_values = param_grid[param1]\n",
    "\n",
    "# # Start plot\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# # Plot a line for each max_depth value\n",
    "# for i, max_depth in enumerate(max_depth_values):\n",
    "#     if max_depth in []:\n",
    "#         continue\n",
    "#     ax[0].plot(n_estimators_values, mean_test_scores_depth[i, :], label=f\"{p1_name}={max_depth}\", marker='o', alpha=0.6)\n",
    "\n",
    "# ax[0].set_title('Mean Test Score vs. n_estimators')\n",
    "# ax[0].set_xlabel('n_estimators')\n",
    "# ax[0].set_ylabel('Mean Test Score (Negative MSE)')\n",
    "# ax[0].legend(title='Max Depth', prop={'size': 10})\n",
    "# ax[0].grid(True)\n",
    "\n",
    "# # Plot a line for each n_estimators value\n",
    "# for i, n_est in enumerate(n_estimators_values):\n",
    "#     ax[1].plot(max_depth_values, mean_test_scores_estimators[i, :], label=f'{p2_name}={n_est}', marker='o', alpha=0.6)\n",
    "\n",
    "# ax[1].set_title('Mean Test Score vs. max_depth')\n",
    "# ax[1].set_xlabel('max_depth')\n",
    "# ax[1].set_ylabel('Mean Test Score (Negative MSE)')\n",
    "# ax[1].legend(title='n_estimators', prop={'size': 10})\n",
    "# ax[1].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)\n",
    "prediction = pipe.predict(test)\n",
    "prediction[prediction < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"log_bike_count\": prediction})\n",
    "\n",
    "# submission = pd.DataFrame({'Id' : submission.index, 'log_bike_count' : prediction})\n",
    "submission = pd.DataFrame({\"Id\": test.index, \"log_bike_count\": prediction})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
